{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA NeMo Retriever Reranking\n",
    "\n",
    "Reranking is a critical piece of high accuracy, efficient retrieval pipelines.\n",
    "\n",
    "Two important use cases:\n",
    "- Combining results from multiple data sources\n",
    "- Enhancing accuracy for single data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with NVIDIA NIMs\n",
    "\n",
    "[ai.nvidia.com](http://ai.nvidia.com) hosts a variety of AI models accessible with an api key and the `langchain-nvidia-ai-endpoints` library. The use cases below operate in this mode by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining results from multiple sources\n",
    "\n",
    "Consider a pipeline with data from a semantic store, such as FAISS, as well as a BM25 store.\n",
    "\n",
    "Each store is queried independently and returns results that the individual store considers to be highly relevant. Figuring out the overall relevance of the results is where reranking comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will search for information about the query `What is the meaning of life?` across a BM25 store and semantic store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"What is the meaning of life?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### BM25 relevant documents\n",
    "\n",
    "Below we assume you have ElasticSearch running with documents stored in a `langchain-index` store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-community elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from langchain_community.retrievers import ElasticSearchBM25Retriever\n",
    "\n",
    "bm25_retriever = ElasticSearchBM25Retriever(\n",
    "    client=elasticsearch.Elasticsearch(\"http://localhost:9200\"),\n",
    "    index_name=\"langchain-index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "bm25_docs = bm25_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic documents\n",
    "\n",
    "Below we assume you have a saved FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-community langchain-nvidia-ai-endpoints faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "embedder = NVIDIAEmbeddings()\n",
    "\n",
    "# De-serialization relies on loading a pickle file.\n",
    "# Pickle files can be modified to deliver a malicious payload that\n",
    "# results in execution of arbitrary code on your machine.\n",
    "# Only perform this with a pickle file you have created and no one\n",
    "# else has modified.\n",
    "allow_dangerous_deserialization=True\n",
    "\n",
    "sem_retriever = FAISS.load_local(\"langchain_index\", embeddings=embeddings\n",
    "                                 allow_dangerous_deserialization=allow_dangerous_deserialization).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sem_docs = sem_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine and rank documents\n",
    "\n",
    "The resulting `docs` will be ordered by their relevance to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "\n",
    "ranker = NVIDIARerank()\n",
    "\n",
    "all_docs = bm25_docs + sem_docs\n",
    "\n",
    "docs = ranker.compress_documents(query=query, documents=all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancing accuracy for single data sources\n",
    "\n",
    "Semantic search with vector embeddings is an efficient way to turn a large corpus of documents into a smaller corpus of relevant documents. This is done by trading accuracy for efficiency. Reranking as a tool adds accuracy back into the search by post-processing the smaller corpus of documents. Typically, ranking on the full corpus is too slow for applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-nvidia-ai-endpoints pgvector psycopg langchain-postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we assume you have Postgresql running with documents stored in a collection named `langchain-index`.\n",
    "\n",
    "We will narrow the collection to 1,000 results and further narrow it to 10 with the reranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "ranker = NVIDIARerank(top_n=10)\n",
    "embedder = NVIDIAEmbeddings()\n",
    "\n",
    "store = PGVector(embeddings=embedder,\n",
    "                 collection_name=\"langchain-index\",\n",
    "                 connection=\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\")\n",
    "\n",
    "subset_docs = store.similarity_search(query, k=1_000)\n",
    "\n",
    "docs = ranker.compress_documents(query=query, documents=subset_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a local NIM\n",
    "\n",
    "[Learn more about NIMs](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)\n",
    "\n",
    "The `NVIDIAEmbeddings` and `NVIDIARerank` classes give you a way to work with local NIMs through `mode` switching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# connect to an embedding NIM running at localhost:2016\n",
    "embedder = NVIDIAEmbeddings().mode(\"nim\", base_url=\"http://localhost:2016/v1\")\n",
    "\n",
    "# connect to a reranking NIM running at localhost:1976\n",
    "ranker = NVIDIARerank().mode(\"nim\", base_url=\"http://localhost:1976/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rerun the examples above with this new `embedder` and `ranker`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
