{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDDVue_1cq6d"
   },
   "source": [
    "# NVIDIA AI Foundation Endpoints \n",
    "\n",
    "> [NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/) give users easy access to NVIDIA hosted API endpoints for NVIDIA AI Foundation Models like Mixtral 8x7B, Llama 2, Stable Diffusion, etc. These models, hosted on the [NVIDIA NGC catalog](https://catalog.ngc.nvidia.com/ai-foundation-models), are optimized, tested, and hosted on the NVIDIA AI platform, making them fast and easy to evaluate, further customize, and seamlessly run at peak performance on any accelerated stack.\n",
    "> \n",
    "> With [NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), you can get quick results from a fully accelerated stack running on [NVIDIA DGX Cloud](https://www.nvidia.com/en-us/data-center/dgx-cloud/). Once customized, these models can be deployed anywhere with enterprise-grade security, stability, and support using [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/).\n",
    "> \n",
    "> These models can be easily accessed via the [`langchain-nvidia-ai-endpoints`](https://pypi.org/project/langchain-nvidia-ai-endpoints/) package, as shown below.\n",
    "\n",
    "This example goes over how to use LangChain to interact with the supported [NVIDIA Reranker Model](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nvolve-40k) for [retrieval-augmented generation](https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/) via the `NVIDIAEmbeddings` class.\n",
    "\n",
    "For more information on accessing the chat models through this api, check out the [ChatNVIDIA](../chat/nvidia_ai_endpoints) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet langchain-nvidia-ai-endpoints\n",
    "# %pip install --upgrade --quiet langchain langchain-community langchain-text-splitters\n",
    "# %pip install --upgrade --quiet faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKcxQMFTcwWi"
   },
   "source": [
    "## Setup\n",
    "\n",
    "**To get started:**\n",
    "\n",
    "1. Create a free account with the [NVIDIA NGC](https://catalog.ngc.nvidia.com/) service, which hosts AI solution catalogs, containers, models, etc.\n",
    "\n",
    "2. Navigate to `Catalog > AI Foundation Models > (Model with API endpoint)`.\n",
    "\n",
    "3. Select the `API` option and click `Generate Key`.\n",
    "\n",
    "4. Save the generated key as `NVIDIA_API_KEY`. From there, you should have access to the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoF41-tNczS3",
    "outputId": "7f2833dc-191c-4d73-b823-7b2745a93a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid NVIDIA_API_KEY already in environment. Delete to reset\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "## API Key can be found by going to NVIDIA NGC -> AI Foundation Models -> (some model) -> Get API Code or similar.\n",
    "## 1K free queries to any endpoint.\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\") \n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l185et2kc8pS"
   },
   "source": [
    "## Initialization\n",
    "\n",
    "Let's list out some of the models we will be using for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='ai-mixtral-8x7b-instruct', model_type='chat', api_type=None, kwargs={'model_name': 'mistralai/mixtral-8x7b-instruct-v0.1', 'max_tokens': 1024}, client='ChatNVIDIA', path='a1e53ece-bff4-44d1-8b13-c009e5bf47f6'),\n",
       " Model(id='mixtral_8x7b', model_type='chat', api_type='aifm', kwargs={}, client='ChatNVIDIA', path='8f4118ba-60a8-4e6b-8574-e38a4067a4a3'),\n",
       " Model(id='ai-embed-qa-4', model_type='embedding', api_type=None, kwargs={'model_name': 'NV-Embed-QA'}, client='NVIDIAEmbeddings', path='09c64e32-2b65-4892-a285-2f585408d118'),\n",
       " Model(id='nvolveqa_40k', model_type='embedding', api_type='aifm', kwargs={}, client='NVIDIAEmbeddings', path='091a03bb-7364-4087-8090-bd71e9277520'),\n",
       " Model(id='ai-rerank-qa-mistral-4b', model_type='ranking', api_type=None, kwargs={'model_name': 'nv-rerank-qa-mistral-4b:1'}, client='NVIDIARerank', path='0bf77f50-5c35-4488-8e7a-f49bb1974af6')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import (\n",
    "    ChatNVIDIA,\n",
    "    NVIDIAEmbeddings,\n",
    "    NVIDIARerank,\n",
    ")\n",
    "\n",
    "all_models = ChatNVIDIA.get_available_models(list_all=True)\n",
    "\n",
    "[m for m in all_models if m.client in (\"NVIDIARerank\", \"NVIDIAEmbeddings\") or \"mixtral\" in m.id.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the list above, we should be able to see the following models:\n",
    "- `ai-mixtral-8x7b-instruct`: A NIM-containerized Mixtral-8x7b model which we will use as our LLM backbone via `ChatNVIDIA`.\n",
    "- `ai-embed-qa-4`: A NIM-containterized query-answer embedding model based on the e5-large architecture which we will use to generate embeddings via `NVIDIAEmbeddings`.\n",
    "- `ai-rerank-qa-mistral-4b`: A NIM-containerized mistral-backed question-answer reranking model which we will use to rank question-answer pairs via `NVIDIARerank`.\n",
    "\n",
    "In this notebook, we will focus on the **Reranking Model** which evaluates the relevance of passages in making decisions about a query. They are a common component of a retrieval-augmented generation pipeline and allow you to access quick relevance scores to help rank, order, filter, or otherwise process your retrieval. \n",
    "\n",
    "Let's initialize these models for use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hbXmJssPdIPX"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "\n",
    "llm = ChatNVIDIA(model=\"ai-mixtral-8x7b-instruct\")\n",
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")\n",
    "reranker = NVIDIARerank(model=\"ai-rerank-qa-mistral-4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc Snippets:\n",
      "'One of the most serious constitutional responsibilities a President has is nominating someone to ser'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 73}, 'type': 'Document'}\n",
      "'As I said last year, especially to our younger transgender Americans, I will always have your back a'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 79}, 'type': 'Document'}\n",
      "'And I know you’re tired, frustrated, and exhausted. \\n\\nBut I also know this. \\n\\nBecause of the progres'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 55}, 'type': 'Document'}\n",
      "'A former top litigator in private practice. A former federal public defender. And from a family of p'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 74}, 'type': 'Document'}\n",
      "'This was a bipartisan effort, and I want to thank the members of both parties who worked to make it '...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 25}, 'type': 'Document'}\n",
      "'So let’s not abandon our streets. Or choose between safety and equal justice. \\n\\nLet’s come together '...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 68}, 'type': 'Document'}\n",
      "'I spoke with their families and told them that we are forever in debt for their sacrifice, and we wi'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 67}, 'type': 'Document'}\n",
      "'And tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fr'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 50}, 'type': 'Document'}\n",
      "'He will never extinguish their love of freedom. He will never weaken the resolve of the free world. '...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 18}, 'type': 'Document'}\n",
      "'Let’s pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and ex'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 53}, 'type': 'Document'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = TextLoader(\"../../modules/state_of_the_union.txt\",).load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "for idx, text in enumerate(texts):\n",
    "    text.metadata[\"id\"] = idx\n",
    "\n",
    "retriever = FAISS.from_documents(texts, embedder).as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"\\nDoc Snippets:\")\n",
    "for doc in docs:\n",
    "    print(repr(doc.page_content[:100])+\"...\")\n",
    "    print({k:v for k,v in doc.dict().items() if k != \"page_content\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What did the president say about Ketanji Brown Jackson\n",
      "\n",
      "Most Relevant Chunks:\n",
      "'One of the most serious constitutional responsibilities a President has is nominating someone to ser'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 73, 'relevance_score': 0.43701171875}, 'type': 'Document'}\n",
      "'A former top litigator in private practice. A former federal public defender. And from a family of p'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 74, 'relevance_score': -7.38671875}, 'type': 'Document'}\n",
      "'I spoke with their families and told them that we are forever in debt for their sacrifice, and we wi'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 67, 'relevance_score': -14.3359375}, 'type': 'Document'}\n",
      "'He will never extinguish their love of freedom. He will never weaken the resolve of the free world. '...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 18, 'relevance_score': -14.7109375}, 'type': 'Document'}\n",
      "'Let’s pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and ex'...\n",
      "{'metadata': {'source': '../../modules/state_of_the_union.txt', 'id': 53, 'relevance_score': -14.8203125}, 'type': 'Document'}\n",
      "\n",
      "'Relevant' Documents:\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "top_docs = reranker.compress_documents(docs, query, top_k=5)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "\n",
    "print(\"\\nMost Relevant Chunks:\")\n",
    "for doc in top_docs:\n",
    "    print(repr(doc.page_content[:100])+\"...\")\n",
    "    print({k:v for k,v in doc.dict().items() if k != \"page_content\"})\n",
    "\n",
    "print(\"\\n'Relevant' Documents:\")\n",
    "for doc in top_docs:\n",
    "    if doc.metadata.get('relevance_score') > 0:\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Documents: [73, 74, 67, 68, 53]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
       " 'result': ' At her confirmation hearing to become an Associate Justice on the Supreme Court, President Joe Biden expressed his strong support for Judge Ketanji Brown Jackson. He praised her extensive qualifications, experience, and temperament, noting that she is \"one of our nation\\'s brightest legal minds\" and \"eminently qualified\" for the role. President Biden emphasized that her confirmation would be a historic milestone, as Judge Jackson would be the first Black woman to serve on the Supreme Court.\\n\\nHere is a quote from the President\\'s statement at the confirmation hearing:\\n\\n\"Judge Jackson’s nomination is a testament to her character and to her brilliant legal mind. She is, without a doubt, one of our nation’s brightest legal minds. One of our most impressive legal scholars. A consensus builder. A devoted public servant. And someone who will be the first Black woman to serve on the United States Supreme Court.\" (Source: White House Briefing Room)\\n\\nPresident Biden encouraged Senators to support her nomination, citing her vast experience, impeccable credentials, and strong commitment to upholding the Constitution and the rule of law. After a successful hearing, Judge Ketanji Brown Jackson is now set to become the next Associate Justice of the Supreme Court.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")\n",
    "print(\"Most Relevant Documents:\", [doc.metadata[\"id\"] for doc in compressed_docs])\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
